{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import re\n",
    "import dlib\n",
    "import timeit\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.pyplot import imshow\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "le = preprocessing.LabelEncoder()\n",
    "oe = preprocessing.OneHotEncoder()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of examples\n",
    "N = 5000\n",
    "# Length of a feature\n",
    "d = 128\n",
    "# ICM iterations\n",
    "NITER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2i(a):\n",
    "        return int(ord(a)-ord('a'))\n",
    "def i2l(i):\n",
    "    if i >= 0:\n",
    "        return chr(i+ord('a'))\n",
    "    else:\n",
    "        return '_'\n",
    "def iors(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError: # if it is a string, return a string\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_OCR(filename, n_features):\n",
    "    F = open(filename)\n",
    "    dataset = {}\n",
    "    dataset['ids'] = []#np.zeros(n_examples, dtype=int)\n",
    "    dataset['labels'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['labelDic'] = {} # To profile the distribution of labels\n",
    "    dataset['next_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['word_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['positions'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['folds'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['features'] = []#np.zeros([n_examples,n_features])\n",
    "    \n",
    "    for str_line in F.readlines():\n",
    "        #line0 = map(iors, filter(None, re.split('\\t', str_line.strip())))\n",
    "        ## ATTENTION: If you are using Python3, use the following line instead\n",
    "        line0 = list(map(iors, filter(None, re.split('\\t', str_line.strip()))))\n",
    "\n",
    "\n",
    "        dataset['ids'].append(int(line0.pop(0)))\n",
    "        dataset['labels'].append(l2i(line0.pop(0))) # The label is converted into integer('a'=>0, 'z'=>25)\n",
    "        if dataset['labels'][-1] in dataset['labelDic']:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] += 1\n",
    "        else:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] = 1\n",
    "            \n",
    "        dataset['next_ids'].append(int(line0.pop(0)))\n",
    "        dataset['word_ids'].append(int(line0.pop(0)))\n",
    "        dataset['positions'].append(int(line0.pop(0)))\n",
    "        dataset['folds'].append(int(line0.pop(0)))\n",
    "        if len(line0) != 128:  # Sanity check of the length\n",
    "            print (len(line0))\n",
    "        dataset['features'].append(line0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structurize(dataset, N, L):\n",
    "    d_features = len(dataset['features'][0])\n",
    "    y = dataset['labels']\n",
    "    X = dataset['features']\n",
    "    next_id = dataset['next_ids']\n",
    "\n",
    "    labels = np.zeros((N, L))\n",
    "    features = np.zeros((N, L*d_features))\n",
    "    \n",
    "    # Extract only one structured example\n",
    "    def extract(iN, loc):\n",
    "        labels[iN] = y[loc:loc+L]\n",
    "        features[iN] = np.array(X[loc:loc+L]).ravel().tolist()\n",
    "        iN += 1\n",
    "        return iN\n",
    "    \n",
    "    iN = 0\n",
    "    iN = extract(iN, 0)\n",
    "    \n",
    "    for key, value in enumerate(y):\n",
    "        if next_id[key] == -1:\n",
    "            iN = extract(iN, key+1)\n",
    "            \n",
    "            if iN == N:\n",
    "                break\n",
    "    \n",
    "    c = list(zip(labels, features))\n",
    "    random.shuffle(c)\n",
    "    labels, features = zip(*c)\n",
    "    \n",
    "    return np.array(labels), np.array(features)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeClassProblem:\n",
    "    C = 1\n",
    "\n",
    "    def __init__(self, samples, labels, L, K, d):\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.num_samples = len(samples)\n",
    "        self.num_dimensions = (L*K*d+1) + (L-1)\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.loss_for_loop = True\n",
    "        \n",
    "    def make_psi(self, x, label):       \n",
    "        psi = dlib.vector()\n",
    "        psi.resize(self.num_dimensions)\n",
    "        psi[0] = x[0] # The bias term\n",
    "\n",
    "        for i in range(self.L):\n",
    "            for k in range(self.K):\n",
    "                if (label[i] == k):\n",
    "                    # i * self.K * self.d + 1= the index of start of current x[i], + 1 due to bias term\n",
    "                    block_start = i * self.K * self.d + 1\n",
    "                    # + k * self.d = the index where x[i] starts in its block (due to K off set)\n",
    "                    offset = k * self.d\n",
    "                    start_index = block_start + offset\n",
    "                    for j in range(self.d):\n",
    "                        psi[start_index + j] = x[i * self.d + 1 + j]\n",
    "        \n",
    "        # The last L-1 elements are the transition variables\n",
    "        for i in range(self.L-1):\n",
    "            if label[i] != label[i+1]:\n",
    "                psi[self.K * self.d * self.L + i] = 1.0\n",
    "            else:\n",
    "                psi[self.K * self.d * self.L + i] = 0.0\n",
    "        \n",
    "        return psi\n",
    "\n",
    "    def get_truth_joint_feature_vector(self, idx):\n",
    "        return self.make_psi(self.samples[idx], self.labels[idx])\n",
    "    \n",
    "    def separation_oracle(self, idx, current_solution):\n",
    "        samp = self.samples[idx]\n",
    "        psi = [0]*self.num_dimensions\n",
    "        max1 = -1e10\n",
    "        max_scoring_label = [0]*self.L # Initialize max_scoring_label for icm search\n",
    "        for k in range(NITER):\n",
    "            for iL in range(self.L):   # Iterate over the window length\n",
    "                for i in range(self.K):# Change different label for the search of a structured label\n",
    "                    tmp_label = max_scoring_label.copy() # New a list to avoid modifying the max_scoring_label\n",
    "                    tmp_label[iL] = i # Take turns to modify the structured label from left to right. The guessed structured label.\n",
    "                    tmp_psi = self.make_psi(samp, tmp_label) # Make a new Psi for the guessed structured label\n",
    "                    score1 = dlib.dot(current_solution, tmp_psi)\n",
    "                    \n",
    "                    loss1 = 0.0\n",
    "                    if self.loss_for_loop:\n",
    "                        for j in range(self.L):\n",
    "                            if self.labels[idx][j] != tmp_label[j]:\n",
    "                                loss1 += 1.0\n",
    "                    else:\n",
    "                        if self.labels[idx] != tmp_label: # Add the conditional \"1\"\n",
    "                            loss1 += 1.0\n",
    "\n",
    "                    if max1 < score1+loss1: # Search for the maximum and update loss, max_scoring_label, and psi\n",
    "                        max1 = score1 + loss1\n",
    "                        loss = loss1\n",
    "                        max_scoring_label[iL] = i\n",
    "                        psi = tmp_psi\n",
    "\n",
    "        return loss, psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(dataset, L, problem, weights, K):\n",
    "    words = defaultdict(lambda: {'features': [], 'labels': [], 'next_ids': []})\n",
    "\n",
    "    # Extract by per word\n",
    "    for i in range(len(dataset['features'])):\n",
    "        word_id = dataset['word_ids'][i]\n",
    "        words[word_id]['features'].append(dataset['features'][i])\n",
    "        words[word_id]['labels'].append(dataset['labels'][i])\n",
    "        words[word_id]['next_ids'].append(dataset['next_ids'][i])\n",
    "\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    next_ids = []\n",
    "\n",
    "    # Only consider the first 1000 words for prediction\n",
    "    words_to_predict = list(words.values())[:1000]\n",
    "\n",
    "    for word in words_to_predict:\n",
    "        word_feats = word['features']\n",
    "        word_labels = word['labels']\n",
    "        word_next_ids = word['next_ids']\n",
    "        word_len = len(word_feats)\n",
    "\n",
    "        if word_len < L:\n",
    "            continue\n",
    "\n",
    "        # For each character position, collect {label: list of scores}\n",
    "        score_votes = [dict() for _ in range(word_len)]\n",
    "\n",
    "        # Slide window with stride = 1\n",
    "        for i in range(0, word_len - L + 1):\n",
    "            x_seq = word_feats[i:i + L]\n",
    "\n",
    "            # ICM initialization\n",
    "            prediction = [0] * L\n",
    "\n",
    "            for _ in range(NITER):\n",
    "                for iL in range(L):\n",
    "                    best_label = 0\n",
    "                    best_score = -1e10\n",
    "                    for label in range(K):\n",
    "                        tmp_label = prediction.copy()\n",
    "                        tmp_label[iL] = label\n",
    "\n",
    "                        # Flatten and add bias before passing to make_psi\n",
    "                        x_flat = [1.0] + np.array(x_seq).ravel().tolist()\n",
    "                        psi = problem.make_psi(x_flat, tmp_label)\n",
    "                        \n",
    "                        score = dlib.dot(weights, psi)\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_label = label\n",
    "                    prediction[iL] = best_label\n",
    "\n",
    "            # Final score for full sequence\n",
    "            x_flat = [1.0] + np.array(x_seq).ravel().tolist()\n",
    "            psi_final = problem.make_psi(x_flat, prediction)\n",
    "            seq_score = dlib.dot(weights, psi_final)\n",
    "\n",
    "            for j in range(L):\n",
    "                abs_pos = i + j\n",
    "                label = prediction[j]\n",
    "                prev_best = score_votes[abs_pos].get(label, -1e10)\n",
    "                score_votes[abs_pos][label] = max(prev_best, seq_score)\n",
    "\n",
    "        # Final prediction: pick label with highest max score at each position\n",
    "        word_pred = []\n",
    "        for label_score_dict in score_votes:\n",
    "            if not label_score_dict:\n",
    "                word_pred.append(0)\n",
    "            else:\n",
    "                best_label = max(label_score_dict.items(), key=lambda x: x[1])[0]\n",
    "                word_pred.append(best_label)\n",
    "\n",
    "        predicted_labels.extend(word_pred)\n",
    "        true_labels.extend(word_labels)\n",
    "        next_ids.extend(word_next_ids)\n",
    "\n",
    "    return predicted_labels, true_labels, next_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_word_accuracy(predicted_labels, true_labels, next_ids):\n",
    "    assert len(predicted_labels) == len(true_labels) == len(next_ids)\n",
    "\n",
    "    pred_words = []\n",
    "    true_words = []\n",
    "\n",
    "    pred_buffer = []\n",
    "    true_buffer = []\n",
    "\n",
    "    for pred, truth, nxt in zip(predicted_labels, true_labels, next_ids):\n",
    "        pred_buffer.append(pred)\n",
    "        true_buffer.append(truth)\n",
    "\n",
    "        if nxt == -1:\n",
    "            pred_words.append(pred_buffer)\n",
    "            true_words.append(true_buffer)\n",
    "            pred_buffer = []\n",
    "            true_buffer = []\n",
    "\n",
    "    assert len(pred_words) == len(true_words)\n",
    "\n",
    "    correct = sum(p == t for p, t in zip(pred_words, true_words))\n",
    "    total = len(true_words)\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(samples, labels, problem, weights, K):\n",
    "    predictions = []\n",
    "    for samp in samples:\n",
    "        prediction = [0]*problem.L # Initialize max_scoring_label for icm search\n",
    "        for k in range(NITER):\n",
    "            for iL in range(problem.L):\n",
    "                max = -1e10\n",
    "                for i in range(K):\n",
    "                    tmp_label = prediction.copy()\n",
    "                    tmp_label[iL] = i\n",
    "                    psi1 = problem.make_psi(samp, tmp_label)\n",
    "                    score = dlib.dot(weights, psi1)\n",
    "\n",
    "                    if max < score:\n",
    "                        max = score\n",
    "                        prediction[iL] = i\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    \n",
    "    errCnt = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != labels[i]:\n",
    "            errCnt += 1\n",
    "\n",
    "    return 1.0-float(errCnt)/float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_OCR('OCRdataset/letter.data', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4000/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.8\n",
    "model_path = 'weights1_82.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 13\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 91.16748304199973 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.6945\n",
      "Test accuracy= 0.6859999999999999\n",
      "Testing time elapsed: 2.735049874998367 s\n",
      "Prediction time elapsed: 23.564398582999274 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2500/2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.5\n",
    "model_path = 'weights1_55.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 13\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 55.737573874999725 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.7048\n",
      "Test accuracy= 0.6839999999999999\n",
      "Testing time elapsed: 6.795320833000005 s\n",
      "Prediction time elapsed: 23.621400500000163 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.2\n",
    "model_path = 'weights1_28.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 13\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 22.375254915999903 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.726\n",
      "Test accuracy= 0.67025\n",
      "Testing time elapsed: 10.902206750000914 s\n",
      "Prediction time elapsed: 23.61164245800137 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4000/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.8\n",
    "model_path = 'weights2_82.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 23\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 1034.7579585420008 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.481\n",
      "Test accuracy= 0.43999999999999995\n",
      "Testing time elapsed: 19.073470875000567 s\n",
      "Prediction time elapsed: 141.46359883399964 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2500/2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.5\n",
    "model_path = 'weights2_55.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 23\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 633.6864789580013 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.5032\n",
      "Test accuracy= 0.4468\n",
      "Testing time elapsed: 47.776799582999956 s\n",
      "Prediction time elapsed: 141.3395536249991 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.2\n",
    "model_path = 'weights2_28.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 23\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 224.20808962499905 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.5840000000000001\n",
      "Test accuracy= 0.44725000000000004\n",
      "Testing time elapsed: 76.43969287499931 s\n",
      "Prediction time elapsed: 141.92162695800107 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4000/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.8\n",
    "model_path = 'weights3_82.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 24\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 2242.6402945829996 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.3335\n",
      "Test accuracy= 0.30500000000000005\n",
      "Testing time elapsed: 45.23605624999982 s\n",
      "Prediction time elapsed: 279.48097662500004 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2500/2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.5\n",
    "model_path = 'weights3_55.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 24\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 1599.3623139170013 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.34840000000000004\n",
      "Test accuracy= 0.2964\n",
      "Testing time elapsed: 114.93170691699925 s\n",
      "Prediction time elapsed: 287.01649533300224 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "labels, features = structurize(dataset, N, L)\n",
    "TRAIN = 0.2\n",
    "model_path = 'weights3_28.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 24\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "nplabels  = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "npsamples = np.hstack([np.ones((N,1)), features]) # Add ones for bias\n",
    "K = len(le.classes_)\n",
    "print ('K=', K)\n",
    "\n",
    "tr_labels  = nplabels[:int(N*TRAIN)].astype(int).tolist()\n",
    "tr_samples = npsamples[:int(N*TRAIN)].astype(int).tolist()\n",
    "te_labels  = nplabels[int(N*TRAIN):].astype(int).tolist()\n",
    "te_samples = npsamples[int(N*TRAIN):].astype(int).tolist()\n",
    "\n",
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 672.6627867909992 s\n"
     ]
    }
   ],
   "source": [
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.398\n",
      "Test accuracy= 0.25775000000000003\n",
      "Testing time elapsed: 178.97353929200108 s\n",
      "Prediction time elapsed: 283.40860166700077 s\n",
      "Full word accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "weights_load = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K))\n",
    "start_test = timeit.default_timer()\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K))\n",
    "end_test = timeit.default_timer()\n",
    "print (\"Testing time elapsed:\", end_test - start_test, \"s\")\n",
    "\n",
    "start_predict = timeit.default_timer()\n",
    "predicted_labels, true_labels, next_ids = predict_words(dataset, L, problem, weights_load, K)\n",
    "end_predict = timeit.default_timer()\n",
    "print (\"Prediction time elapsed:\", end_predict - start_predict, \"s\")\n",
    "full_accuracy = full_word_accuracy(predicted_labels, true_labels, next_ids)\n",
    "print (\"Full word accuracy:\", full_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
